{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7ed08b1-fe5b-4aee-a7c7-ea9ea5edb55e",
   "metadata": {},
   "source": [
    "# Results & Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5daba-c430-48d7-8cae-9d3103ba9ef8",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "### Distribution of target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916b0ff-2627-4005-822b-ff71ddbd1eee",
   "metadata": {},
   "source": [
    "In total there are 658k entries with no missing entries. The first dataset has more rows than the second one, making an aggregation of the laims necessary before merging the two datasets to arrive at the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d368f-31ae-4200-bfec-4dac838cb4d2",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"ClaimNb_histogram.png\" width=\"800\"/> | Number of claims within the insurance period of 12 months tends naturally to be biased towards zero, as on average, accidents do not occur that often. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba6988-3054-4458-9b84-dcf947ecc24f",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"BonusMalus_histogram.png\" width=\"600\"/> | Average is 59 which is in accordance with the number of claims. as a low number such as 50 indicates no or few claims. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb0308-87cc-48c0-9077-d7841652a0e8",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"Density_histogram.png\" width=\"1200\"/> | Number of inhabitants per square kilometer in the location of the insured. Heavily biased towards a low density already indicating a bias or attractiveness of the products of the insurance towards a certain demographic |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6513a41-6694-4967-bc6f-9d3e7fda2cc0",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"DrivAge_histogram.png\" width=\"450\"/> | The drivers age is normally distributed with the average age being 45 years |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee4755-c58c-46c6-9162-84649f2d0163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7be0ca0b-cedf-4dc8-839b-61ef4565e288",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"VehAge_histogram.png\" width=\"700\"/> | The vehicles age is also skewed to the left indicating that most drivers holding a car insurance have a relatively new car |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786d880-6bd8-47c2-a441-ddbe853ad6d9",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"Exposure_histogram.png\" width=\"1100\"/> | The duration of holding the insruance in years is on average 0.5 meaning half a year though the data is skewed towards one meaning that a lot of customers hold a policy for a year |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990e5a16-40a4-4529-9d3b-06727a5fb209",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"VehPower_histogram.png\" width=\"1200\"/> | TThe vehicle power is also skewed towards the left meaning most insured cars do not posess a lot of horse power and potentially bear less of a risk which is reflected in the low number of claims |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9c97b-5c3a-4c06-8400-9e3e95581e0b",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"total_claim_amount_histogram.png\" width=\"1000\"/> | The claim amount is also near zero with and average of XX this is as expected for insurance data, as the business case is around the fact of having little claims |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08662762-0aba-4de5-ae67-4b0142e5c2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c60edec-1f95-4ac9-827c-fe00c90984e0",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f71d52-5b8f-4fe2-8b42-641466ebaca8",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"Area_bar_chart.png\" width=\"500\"/> | The majority of insured come from area C (200k) and the least from F (25k) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2bba3-a418-4c79-b0c3-075ac3ceb8bb",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"Region_bar_chart.png\" width=\"1000\"/> | 160k of all insured come from region R24. R24, R82, R93 and R11 account for the majority of regions. I think about 90% of all insured come from these areas |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ff81e3-076b-4289-a34a-3aaa66fed87a",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"VehBrand_bar_chart.png\" width=\"600\"/> | The vast majority of cars around 160k each come from VehBrans B1, B2, B12. B3 has around 55k |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf336c-8f66-4cd7-9ec2-23600244a5a8",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"VehGas_bar_chart.png\" width=\"900\"/> | The split between regular and diesel is approx 50:50 which is expected and reflects the global market of types of gas used by vehicles|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5996b1e5-9cf5-4899-8083-76529c349f92",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"target_histogram.png\" width=\"900\"/> | As the target value is defined as the amount claimed per insured divided by the exposure a heavily left skew is expected. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda3206-0173-4cf3-ba95-8a76307093fb",
   "metadata": {},
   "source": [
    "Overall it can be said that nearly all data is biased apart from the drivers age. This was done by comparing average and mean of all features where possible. There is also a lot of categorical data which will result in a large dataset when training the model, which makes further investigation using PCA necessary. It will also cause issues for the machine learning model as skewness will bias the model towards a target value of zero. Due to this imbalance in data it is expected that the models will not perform great in general"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c9068-abf2-4c26-a3d8-8b886a393a7c",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"correlation_matrix_combined.png\" width=\"1200\"/> | As the target value is defined as the amount claimed per insured divided by the exposure a heavily left skew is expected. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38098d9c-21e3-45c0-9c06-b2baabf4667d",
   "metadata": {},
   "source": [
    "The correlation matrix reveals a strong positive correlation (≈ 0.68) between total_claim_amount and target, which is expected since the target variable is defined as the total claim amount divided by the exposure.\n",
    "Interestingly, there's a moderate negative correlation (-0.48) between DrivAge (driver’s age) and BonusMalus, indicating that younger drivers tend to have higher Bonus-Malus values (i.e., less favorable insurance ratings or higher premiums). This aligns with real-world insurance practice, where less experienced or younger drivers are typically considered higher risk.\n",
    "Aside from this, most features show low or negligible pairwise correlation, suggesting that there is little multicollinearity in the dataset. This can be advantageous for machine learning models, as it implies that most features carry distinct information and thus should be retained unless domain-specific knowledge or feature selection suggests otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc1c53-0671-4348-8a62-b06e842a7103",
   "metadata": {},
   "source": [
    "## Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbab94-73d9-404a-ab5c-044cf39066ad",
   "metadata": {},
   "source": [
    "Feature engineering of the categorical values was done through one hot encoding. This increases the number of training features drastically (high dimensionality). That is why I decided to additionally conduct PCA to potentially optimise the number of features for a more accurate modelling of the problem in machine learning. For this I used feature scaling using the standard scaler and was necessary because PCA is sensitive to feature scaling.\n",
    "Teh results on the PCA can be seen below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c57a9-0b56-40f5-a7c7-6c81cb6b1c93",
   "metadata": {},
   "source": [
    "| Component | Explained Variance |\n",
    "|------|-------------|\n",
    "| PC1 |  0.055289892|\n",
    "| PC2 |  0.042729385|\n",
    "| PC3 |  0.037880697|\n",
    "| PC4 |  0.032881772|\n",
    "| PC5 |  0.032087697|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f471902-e699-4349-8017-7fed853cfcd2",
   "metadata": {},
   "source": [
    "Each principal component explains only a small portion of the total variance in the data. Even after 5 components only  about 20% of the total variance (5.5 + 4.3 + 3.8 + 3.3 + 3.2) is explained.\n",
    "That implies the data is not strongly reducible — no low-dimensional linear subspace captures most of the variance. As a result, applying PCA here may discard useful information, which can explain why your model performance dropped when using PCA features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e126c95-33dc-47fe-b706-20be29335640",
   "metadata": {},
   "source": [
    "## 🧪 Model Training Summary\n",
    "### Model Choice\n",
    "For the modelling task, I explored several approaches: Random Forest, XGBoost, and a Neural Network. While I also briefly considered a Generalised Linear Model (GLM) — a standard method in insurance contexts — I found it challenging to tune effectively for this dataset and ultimately excluded it from further analysis due to suboptimal performance.\n",
    "\n",
    "- **Random Forest**: A robust ensemble method capable of capturing non-linear relationships in the data. It tends to perform well with limited parameter tuning and provides feature importance insights.\n",
    "- **XGBoost**: A gradient boosting technique that often yields higher accuracy than Random Forest, especially for complex non-linear problems. It is also considerably faster due to its efficient implementation. And I used it to improve the results obtained with Random Forest.\n",
    "- **Neural Network**: Particularly suited for modelling highly non-linear interactions. While it requires more tuning and computational resources, it has the potential to learn intricate patterns in the data.\n",
    "\n",
    "Each model was evaluated on both the full set of features and a reduced feature set derived from PCA, with corresponding performance metrics assessed and documented. I quickly realised that training with selected features did not yield good results which is why I removed the code at some stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f064d24-b0d0-48c4-89cb-3fbc8b163069",
   "metadata": {},
   "source": [
    "### Metrics (R², MAE, Loss)\n",
    "\n",
    "To evaluate model performance, I primarily focused on the **R² score (coefficient of determination)**. This metric indicates how well the model's predictions explain the variance in the target variable. An R² of 1.0 means perfect predictions, while values close to 0 (or negative) suggest the model isn't capturing the underlying structure of the data.\n",
    "\n",
    "R² is particularly helpful in regression tasks like this one, where the goal is to predict a continuous target variable. It offers an intuitive understanding of model quality — essentially answering how much better is the model than simply predicting the mean.\n",
    "\n",
    "Although additional metrics like **Mean Absolute Error (MAE)** were also calculated during training, the focus remained on maximising R². MAE, which measures the average magnitude of prediction errors, is useful for interpretability (it’s in the same units as the target), but R² was better suited for comparing models and assessing generalisation performance in this context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b157c4f-0728-4025-a6d0-1cab7fa7d1ee",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "Hyperparameter tuning was done for all models. A summary of the best model parameters for each machine learning models can be found below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cc6b21-c280-49ff-ae2d-cea762ff5755",
   "metadata": {},
   "source": [
    "<h3>Random Forest</h3>\n",
    "\n",
    "<table style=\"width:100%;\">\n",
    "<tr>\n",
    "<td style=\"vertical-align: top; width: 50%; text-align: left;\">\n",
    "\n",
    "<b>Metrics</b><br>\n",
    "\n",
    "<table>\n",
    "<tr><th>Metric</th><th>Train</th><th>Validation</th></tr>\n",
    "<tr><td>MAE</td><td>108.20</td><td>144.61</td></tr>\n",
    "<tr><td>R² Score</td><td>0.6808</td><td>0.5156</td></tr>\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "<td style=\"vertical-align: top; width: 50%; text-align: left;\">\n",
    "\n",
    "<b>Best Parameters</b><br>\n",
    "- <code>max_depth</code>: 10<br>\n",
    "- <code>min_samples_leaf</code>: 2<br>\n",
    "- <code>min_samples_split</code>: 5<br>\n",
    "- <code>n_estimators</code>: 50<br>\n",
    "- <code>random_state</code>: 42\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3fc32e-a8ff-479e-a434-54d9661244c7",
   "metadata": {},
   "source": [
    "<h3>XGBoost</h3>\n",
    "\n",
    "<table style=\"width:100%;\">\n",
    "<tr>\n",
    "<td style=\"vertical-align: top; width: 50%; text-align: left;\">\n",
    "\n",
    "<b>Metrics</b><br>\n",
    "\n",
    "<table>\n",
    "<tr><th>Metric</th><th>Train</th><th>Validation</th></tr>\n",
    "<tr><td>MAE</td><td>164.81</td><td>281.69</td></tr>\n",
    "<tr><td>R² Score</td><td>0.9890</td><td>0.5240</td></tr>\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "<td style=\"vertical-align: top; width: 50%; text-align: left;\">\n",
    "\n",
    "<b>Best Parameters</b><br>\n",
    "- <code>max_depth</code>: 3<br>\n",
    "- <code>n_estimators</code>: 200<br>\n",
    "- <code>learning_rate</code>: 0.1<br>\n",
    "- <code>subsample</code>: 1.0<br>\n",
    "- <code>colsample_bytree</code>: 1.0<br>\n",
    "- <code>random_state</code>: 42\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e13394-5496-4d33-88bc-e5fb9be72563",
   "metadata": {},
   "source": [
    "<h3>Neural Network</h3>\n",
    "\n",
    "<table style=\"width:100%;\">\n",
    "<tr>\n",
    "<td style=\"vertical-align: top; width: 50%; text-align: left;\">\n",
    "\n",
    "<b>Metrics</b><br>\n",
    "\n",
    "<table>\n",
    "<tr><th>Metric</th><th>Train</th><th>Validation</th></tr>\n",
    "<tr><td>MAE</td><td>295.40</td><td>381.16</td></tr>\n",
    "<tr><td>R² Score</td><td>0.6089</td><td>0.5267</td></tr>\n",
    "<tr><td>MSE Loss</td><td>572M</td><td>437M</td></tr>\n",
    "</table>\n",
    "\n",
    "</td>\n",
    "<td style=\"vertical-align: top; width: 50%; text-align: left;\">\n",
    "\n",
    "<b>Architecture</b><br>\n",
    "- Layers: <code>128 → 64 → 32 → 16 → 8 → 4 → 1</code><br>\n",
    "- Dropout: <code>0.1</code> after each hidden layer<br>\n",
    "- Optimizer: <code>Adam</code><br>\n",
    "- Learning rate: <code>0.0003</code>\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701580c-43db-4117-98bc-86f357fa20b9",
   "metadata": {},
   "source": [
    "| Plot | Explanation |\n",
    "|------|-------------|\n",
    "| <img src=\"model_val_r2_comparison.png\" width=\"1600\"/> | All models reach similar performance, plateauing around an R² of 0.52–0.53. This suggests a common limitation likely driven by data quality, feature skewness, or target variability.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7ef8e9-b2b1-47ef-976b-79b3ef60e2aa",
   "metadata": {},
   "source": [
    "### 🔍 Observations\n",
    "\n",
    "- **PCA-based Feature Selection**: Applying PCA led to a significant drop in model performance. While PCA is often used for dimensionality reduction, in this case it likely discarded predictive signals—especially in a dataset with many categorical features and skewed distributions. The top PCA components only explained a small portion of the variance (~5% per component), making PCA unsuitable for this task.\n",
    "\n",
    "- **XGBoost Tendency to Overfit**: XGBoost achieved a very high training R² (~0.99) but performed only slightly better than the Random Forest on validation data. This indicates overfitting. Although early stopping was used, further hyperparameter tuning (e.g. regularisation terms, smaller trees, or subsample ratios) could help mitigate this effect.\n",
    "\n",
    "- **Random Forest Stability**: The Random Forest model delivered stable results with a training R² of ~0.68 and a validation R² of ~0.52, indicating a reasonable balance between bias and variance. It’s also relatively robust to unscaled and skewed features, which suits the dataset characteristics.\n",
    "\n",
    "- **Neural Network Performance**: The neural network slightly outperformed the other models in terms of validation R² (approx. 0.53), with a reasonable training R² (~0.61), showing less overfitting than XGBoost. However, the model requires more complex tuning and is sensitive to scale and architecture. The achieved performance still reflects the limitations of the data.\n",
    "\n",
    "- **Overall Model Limitations**: Despite hyperparameter tuning, none of the models exceeded a validation R² of 0.53. This reflects the underlying difficulty of the problem, particularly due to the **high skewness** of input features and the **target variable**. Since many policyholders have zero claims, the target distribution is heavily imbalanced.\n",
    "\n",
    "- **Opportunities for Improvement**:\n",
    "  - Advanced **feature engineering** (e.g. log-transforms, custom risk scores)\n",
    "  - **Data enrichment** (e.g. external socio-economic data, weather, road conditions)\n",
    "  - **Segmentation**: Separate models for claimants vs. non-claimants could improve performance due to the zero-inflated nature of the target.\n",
    "  - **GLMs** and other insurance-specific models were omitted due to limited familiarity but would be worth exploring, especially since they are interpretable and tailored to actuarial tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a67581-3c28-4d6e-94ae-83dd6ecc264d",
   "metadata": {},
   "source": [
    "## Reflections on Model Performance and Limitations\n",
    "\n",
    "While several models were trained (Random Forest, XGBoost, and Neural Networks), performance across the board remained relatively low, with R² peaking at ~0.53. There are several reasons for this:\n",
    "\n",
    "- **Skewness of Input Features**: Many input variables are heavily skewed, which can challenge models—especially in learning the tails of the distribution. Most policyholders do not submit a claim, making the dataset sparse in high-target cases.\n",
    "- **Impact on R²**: R² penalises large errors more heavily. Thus, if a model underpredicts rare, high-value claims, the metric drops significantly—even if it performs well on the majority low-value range. This suggests that the model may perform well on the dense central region but poorly on the business-critical outliers.\n",
    "- **Choice of Models**: While the selected models are strong general-purpose regressors, domain-specific models like GLMs are most likely better suited for insurance contexts. These were briefly explored but ultimately omitted due to unfamiliarity.\n",
    "- **Need for Feature Engineering**: Further improvements could come from more advanced feature engineering, transforming variables to reduce skewness, or even segmenting the modelling task (e.g. claim prediction as a classification + regression hybrid).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672be6f-08a7-4dec-a0b8-9a7175a82ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
